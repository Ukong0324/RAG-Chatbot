# RAG-Chatbot Notes (Markdown)

This document is intended as a **test corpus** for the RAG-Chatbot project.
It includes definitions, rules, and small Q&A blocks that are easy to retrieve and cite.

---

## Project Principles

### Priority Order
1. Hallucination minimization
2. Accuracy
3. Speed

### Non-Negotiable Rule
If the system cannot find sufficient evidence in the provided documents, it must refuse **without calling the LLM**.

**Refusal message:**
> I could not find sufficient evidence in the provided documents (PDF/TXT) to answer your question.

---

## Evidence Gating (Score-Based)

The chat CLI uses a lexical overlap score between the **question tokens** and the **top retrieved chunks**.

### Parameters (chat.ts)
- `MIN_OVERLAP_RATIO`  
  Minimum fraction of query tokens that appear in the top sources used for scoring.
- `MIN_MATCHED_TOKENS`  
  Minimum number of distinct query tokens that must be found in the evidence text.
- `TOP_SOURCES_FOR_SCORING`  
  How many top chunks are concatenated to build the scoring "haystack".

### Example Defaults
- `MIN_OVERLAP_RATIO = 0.45`
- `MIN_MATCHED_TOKENS = 1`
- `TOP_SOURCES_FOR_SCORING = 3`

### Tuning Guide
If the bot refuses too often (false negatives):
- lower `MIN_OVERLAP_RATIO`
- or lower `MIN_MATCHED_TOKENS`
- or increase `TOP_SOURCES_FOR_SCORING`

If the bot answers irrelevant questions (false positives):
- raise `MIN_OVERLAP_RATIO`
- or raise `MIN_MATCHED_TOKENS`
- or decrease `TOP_SOURCES_FOR_SCORING`

---

## Chunking Strategy

We split documents using a recursive character splitter.

### Splitter Settings (splitter.ts)
- `chunkSize = 1200`
- `chunkOverlap = 200`

### Rationale
- Smaller chunks: better precision, fewer irrelevant matches.
- Overlap: reduces boundary issues for definitions and examples.

---

## Citations

Citations are printed deterministically by the CLI (not generated by the model).

### Format
- With page number: `[filename p.N]`
- Without page number: `[filename]`

### Metadata Constraints (Chroma)
Chroma metadata values must be:
- `string`, `number`, `boolean`, or `null`

Objects/arrays must be stringified before storing.

---

## Ingestion Pipeline

### Steps
1. Load documents from `DATA_DIR` (PDF/TXT/MD).
2. Normalize metadata:
   - `filename`
   - `source`
   - `page` (optional, for PDFs)
3. Split into chunks.
4. Create embeddings client-side.
5. Upsert into Chroma collection.

### Reset Option
When `RESET_COLLECTION=true`, ingestion deletes the collection before upserting.

---

## Quick Retrieval Tests

Use these queries to validate retrieval and citations.

### In-domain queries (should answer)
- What is evidence gating?
- What does MIN_OVERLAP_RATIO control?
- What are the splitter defaults?
- What is the refusal message?

### Out-of-domain queries (should refuse)
- Can I sleep?
- How to cook pasta?
- What is the capital of France?

Expected behavior:
- The bot should refuse if it cannot find sufficient evidence in the corpus.

---

## Mini Q&A (for easy citation)

### Q: What is evidence gating?
A: A pre-generation check that decides whether the retrieved context is relevant enough to answer. If not, the system refuses without calling the LLM.

### Q: Why do we print citations in code instead of asking the model?
A: Deterministic citations reduce hallucination risk and keep output stable across model updates.

### Q: What is the main benefit of chunk overlap?
A: It preserves continuity across chunk boundaries, improving retrieval for definitions and examples.

---

## Glossary

- **RAG**: Retrieval-Augmented Generation; generate answers using retrieved documents.
- **Chunk**: A split segment of text stored and retrieved from the vector database.
- **Embedding**: A numeric vector representation used for similarity search.
- **Vector Store**: A database optimized for storing vectors and performing similarity search.
- **Grounding**: Ensuring the answer is supported by retrieved sources.
